{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression task - Diamond price training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53940, 7)\n",
      "[[  326.]\n",
      " [  326.]\n",
      " [  327.]\n",
      " ..., \n",
      " [ 2757.]\n",
      " [ 2757.]\n",
      " [ 2757.]]\n"
     ]
    }
   ],
   "source": [
    "# columns 1, 5, 6, 8, 9, 10 have numerical variables\n",
    "# column 7 contains the target: diamond price\n",
    "diamonds_data = np.genfromtxt('diamonds.csv', delimiter=\",\", skip_header=1,\n",
    "                       usecols=(1, 5, 6, 8, 9, 10, 7))\n",
    "\n",
    "print(diamonds_data.shape)\n",
    "print(diamonds_data[:, np.newaxis, 6]) # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(diamonds_data[:, 0:6])\n",
    "target_df = pd.DataFrame(diamonds_data[:, np.newaxis, 6])\n",
    "\n",
    "#print(diamonds_features_df)\n",
    "#print(diamonds_target_df)\n",
    "\n",
    "features = diamonds_data[:, 0:6]\n",
    "target = diamonds_data[:, np.newaxis, 6]\n",
    "\n",
    "m, n = features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression implementation\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The general form of the cost function is\n",
    "\n",
    "$$\n",
    "h_\\theta\\left(x\\right) = \\theta_0 x_0 + \\theta_1 x_1 +\n",
    "    \\theta_2 x_2 + \\cdots + \\theta_n x_n \n",
    "$$\n",
    "\n",
    "This can be also interpreted as the dot product betweeen the\n",
    "parameters array and the features array\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_0 && x_1 && x_2 && \\cdots && x_n\n",
    "\\end{bmatrix},~~\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 && \\theta_1 && \\theta_2&& \\cdots && \\theta_n\n",
    "\\end{bmatrix},\n",
    "\\\\\n",
    "h_\\theta\\left(x\\right) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "This dot product can be calculated with 2 methods in\n",
    "Numpy:\n",
    "\n",
    "```python\n",
    ">>> numpy.dot(parameters, features)\n",
    "...\n",
    ">>> parameters @ features\n",
    "```\n",
    "\n",
    "The later method will be used, since it is cleaner\n",
    "than the former.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "The main formula for the iteration steps in the\n",
    "Gradient Descent algorithm is\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_j^{\\left(i\\right)}}\n",
    "$$\n",
    "\n",
    "for all $j = 0, 1, ..., n$, where\n",
    "\n",
    "* $i$ is the index of the data example,\n",
    "* $j$ is the index of the parameter,\n",
    "* $m$ is the number of data examples and\n",
    "* $n$ is the number of parameters.\n",
    "\n",
    "We define a difference $k$ between the cost and the target,\n",
    "such that\n",
    "\n",
    "$$\n",
    "k^{\\left(i\\right)} = h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)} ~\n",
    "\\implies\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{k^{\\left(i\\right)} ~ x_j^{\\left(i\\right)}}\n",
    "$$\n",
    "\n",
    "Note that the sum\n",
    "$\\sum_{i=1}^{m}{k^{\\left(i\\right)} ~ x_j^{\\left(i\\right)}}$\n",
    "is also a dot product, and we can simplify even more the\n",
    "formula to\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\left(\n",
    "    k \\cdot x_j\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   388.56919009  15950.864914    14957.42060122   1881.74363742\n",
      "   1881.12496339   1163.73252324]\n",
      "[  -148926.04888529 -11511033.03973183 -10715112.47115523\n",
      "  -1068654.51532792  -1069251.10863625   -659943.80171708]\n",
      "[  1.07231926e+08   8.27150391e+09   7.69967437e+09   7.68383965e+08\n",
      "   7.68809352e+08   4.74513345e+08]\n",
      "[ -7.70543130e+10  -5.94372770e+12  -5.53282300e+12  -5.52143850e+11\n",
      "  -5.52449530e+11  -3.40974873e+11]\n",
      "[  5.53695985e+13   4.27103690e+15   3.97576948e+15   3.96758882e+14\n",
      "   3.96978538e+14   2.45017326e+14]\n",
      "[ -3.97874213e+16  -3.06907670e+18  -2.85690378e+18  -2.85102534e+17\n",
      "  -2.85260374e+17  -1.76064263e+17]\n",
      "[  2.85903987e+19   2.20537354e+21   2.05291058e+21   2.04868645e+20\n",
      "   2.04982066e+20   1.26516052e+20]\n",
      "[ -2.05444552e+22  -1.58473474e+24  -1.47517808e+24  -1.47214271e+23\n",
      "  -1.47295773e+23  -9.09117570e+22]\n",
      "[  1.47628106e+25   1.13875684e+27   1.06003174e+27   1.05785059e+26\n",
      "   1.05843624e+26   6.53272640e+25]\n",
      "[ -1.06082432e+28  -8.18286556e+29  -7.61716369e+29  -7.60149039e+28\n",
      "  -7.60569877e+28  -4.69427890e+28]\n"
     ]
    }
   ],
   "source": [
    "# this is our model, what we want to have as a final result\n",
    "params = np.array([10.0, 10.0, 10.0, 10.0, 10.0, 10.0])\n",
    "\n",
    "lrate = 0.1\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    k_diff = [(params @ features[i, :]) - target[i, 0]\n",
    "              for i in range(m)]\n",
    "\n",
    "    for j in range(n):\n",
    "        params[j] = params[j] - lrate / m * (k_diff @ features[:, j])\n",
    "\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equation\n",
    "\n",
    "$$\n",
    "\\theta = {\\left( X^T X \\right)}^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9533.9516298 ]\n",
      " [   28.29107034]\n",
      " [  -18.82200979]\n",
      " [ -522.62997672]\n",
      " [  182.32945466]\n",
      " [ -676.7502049 ]]\n"
     ]
    }
   ],
   "source": [
    "features_t = features.transpose()\n",
    "\n",
    "normal_params = np.linalg.inv(features_t @ features) @ features_t @ target\n",
    "\n",
    "print(normal_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
