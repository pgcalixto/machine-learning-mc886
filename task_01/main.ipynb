{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression task - Diamond price training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53940, 7)\n",
      "[[  326.]\n",
      " [  326.]\n",
      " [  327.]\n",
      " ..., \n",
      " [ 2757.]\n",
      " [ 2757.]\n",
      " [ 2757.]]\n"
     ]
    }
   ],
   "source": [
    "# columns 1, 5, 6, 8, 9, 10 have numerical variables\n",
    "# column 7 contains the target: diamond price\n",
    "diamonds_data = np.genfromtxt('diamonds.csv', delimiter=\",\", skip_header=1,\n",
    "                       usecols=(1, 5, 6, 8, 9, 10, 7))\n",
    "\n",
    "print(diamonds_data.shape)\n",
    "print(diamonds_data[:, np.newaxis, 6]) # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_features_df = pd.DataFrame(diamonds_data[:, 0:6])\n",
    "diamonds_target_df = pd.DataFrame(diamonds_data[:, np.newaxis, 6])\n",
    "\n",
    "#print(diamonds_features_df)\n",
    "#print(diamonds_target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression implementation\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The general form of the cost function is\n",
    "\n",
    "$$\n",
    "h_\\theta\\left(x\\right) = \\theta_0 x_0 + \\theta_1 x_1 +\n",
    "    \\theta_2 x_2 + \\cdots + \\theta_n x_n \n",
    "$$\n",
    "\n",
    "This can be also interpreted as the dot product betweeen the parameters array\n",
    "and the features array\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_0 && x_1 && x_2 && \\cdots && x_n\n",
    "\\end{bmatrix},~~\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 && \\theta_1 && \\theta_2&& \\cdots && \\theta_n\n",
    "\\end{bmatrix},\n",
    "\\\\\n",
    "h_\\theta\\left(x\\right) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "We will use this dot product to calculate the cost function throughout the code,\n",
    "by calling\n",
    "\n",
    "```python\n",
    "numpy.dot(parameters, features)\n",
    "```\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "The main formula for the iteration steps in the Gradient Descent algorithm is\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_j^{\\left(i\\right)}}\n",
    "$$\n",
    "\n",
    "for all $j = 0, 1, ..., n$, $j$ being the index of the parameter, and\n",
    "$i$ being the index of the data example.\n",
    "\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_0^{\\left(i\\right)}}\n",
    "\\\\\n",
    "\\theta_1 = \\theta_1 - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_1^{\\left(i\\right)}}\n",
    "\\\\\n",
    "\\theta_2 = \\theta_2 - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_2^{\\left(i\\right)}}\n",
    "\\\\\n",
    "...\n",
    "$$\n",
    "\n",
    "We define a difference between the cost and the target, $k$, such that\n",
    "$$\n",
    "k^{\\left(i\\right)} = h_\\theta\\left(x^{\\left(i\\right)}\\right) - y^{\\left(i\\right)}\n",
    "$$\n",
    "It can be noted that $k_i$ is used repeatedly among all the\n",
    "parameter calculations. For this reason, we will calculate all the\n",
    "$k_i$ just once and use it for all parameters.\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{k^{\\left(i\\right)} ~ x_j^{\\left(i\\right)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our model, what we want to have as a final result\n",
    "parameters = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# this is the number of data examples that we have. for this dataset: 53940\n",
    "data_size = diamonds_target_df.shape[0]\n",
    "\n",
    "k_diff = [np.dot(parameters, diamonds_features_df.iloc[i, :]) - diamonds_target_df.iloc[i, 0]\n",
    "          for i in range(data_size)]\n",
    "\n",
    "#print(k_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
