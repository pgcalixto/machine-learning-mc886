{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression task - Diamond price training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53940, 7)\n",
      "[[  326.]\n",
      " [  326.]\n",
      " [  327.]\n",
      " ..., \n",
      " [ 2757.]\n",
      " [ 2757.]\n",
      " [ 2757.]]\n"
     ]
    }
   ],
   "source": [
    "# columns 1, 5, 6, 8, 9, 10 have numerical variables\n",
    "# column 7 contains the target: diamond price\n",
    "diamonds_data = np.genfromtxt('diamonds.csv', delimiter=\",\", skip_header=1,\n",
    "                       usecols=(1, 5, 6, 8, 9, 10, 7))\n",
    "\n",
    "print(diamonds_data.shape)\n",
    "print(diamonds_data[:, np.newaxis, 6]) # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(diamonds_data[:, 0:6])\n",
    "target_df = pd.DataFrame(diamonds_data[:, np.newaxis, 6])\n",
    "\n",
    "#print(diamonds_features_df)\n",
    "#print(diamonds_target_df)\n",
    "\n",
    "features = diamonds_data[:, 0:6]\n",
    "target = diamonds_data[:, np.newaxis, 6]\n",
    "\n",
    "m, n = features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression implementation\n",
    "\n",
    "### Cost function\n",
    "\n",
    "The general form of the cost function is\n",
    "\n",
    "$$\n",
    "h_\\theta\\left(x\\right) = \\theta_0 x_0 + \\theta_1 x_1 +\n",
    "    \\theta_2 x_2 + \\cdots + \\theta_n x_n \n",
    "$$\n",
    "\n",
    "This can be also interpreted as the dot product betweeen the\n",
    "parameters array and the features array\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_0 && x_1 && x_2 && \\cdots && x_n\n",
    "\\end{bmatrix},~~\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 && \\theta_1 && \\theta_2&& \\cdots && \\theta_n\n",
    "\\end{bmatrix},\n",
    "\\\\\n",
    "h_\\theta\\left(x\\right) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "This dot product can be calculated with 2 methods in\n",
    "Numpy:\n",
    "\n",
    "```python\n",
    ">>> numpy.dot(parameters, features)\n",
    "...\n",
    ">>> parameters @ features\n",
    "```\n",
    "\n",
    "The later method will be used, since it is cleaner\n",
    "than the former.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "The main formula for the iteration steps in the\n",
    "Gradient Descent algorithm is\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{\\left(h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)}\\right) x_j^{\\left(i\\right)}}\n",
    "$$\n",
    "\n",
    "for all $j = 0, 1, ..., n$, where\n",
    "\n",
    "* $i$ is the index of the data example,\n",
    "* $j$ is the index of the parameter,\n",
    "* $m$ is the number of data examples and\n",
    "* $n$ is the number of parameters.\n",
    "\n",
    "We define a difference $k$ between the cost and the target,\n",
    "such that\n",
    "\n",
    "$$\n",
    "k^{\\left(i\\right)} = h_\\theta\\left(x^{\\left(i\\right)}\\right) -\n",
    "    y^{\\left(i\\right)} ~\n",
    "\\implies\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\n",
    "    \\sum_{i=1}^{m}{k^{\\left(i\\right)} ~ x_j^{\\left(i\\right)}}\n",
    "$$\n",
    "\n",
    "Note that the sum\n",
    "$\\sum_{i=1}^{m}{k^{\\left(i\\right)} ~ x_j^{\\left(i\\right)}}$\n",
    "is also a dot product, and we can simplify even more the\n",
    "formula to\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\left(\n",
    "    k \\cdot x_j\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our model, what we want to have as a final result\n",
    "params = np.array([10.0, 10.0, 10.0, 10.0, 10.0, 10.0])\n",
    "\n",
    "lrate = 0.1\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    k_diff = [(params @ features[i, :]) - target[i, 0]\n",
    "              for i in range(m)]\n",
    "\n",
    "    for j in range(n):\n",
    "        params[j] = params[j] - lrate / m * (k_diff @ features[:, j])\n",
    "\n",
    "    print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
